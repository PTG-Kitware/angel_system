#
# System configuration to run the ANGEL system for the 2022/11 PI meeting and
# Evaluation 1.
#

name: 2023-10-eval-live
root: <%= ENV["ANGEL_WORKSPACE_DIR"] %>

# Optional tmux socket
# socket_name: foo

# Note that the pre and post options have been deprecated and will be replaced by
# project hooks.

# Project hooks

# Runs on project start, always
# on_project_start: command
on_project_start: |
  export ROS_NAMESPACE=${ROS_NAMESPACE:-/kitware}
  export HL2_IP=${HL2_IP:-172.20.10.12}
  export CONFIG_DIR=${ANGEL_WORKSPACE_DIR}/config
  export BERKELEY_CONFIG_DIR=${ANGEL_WORKSPACE_DIR}/angel_system/berkeley/configs
  export NODE_CONFIG_DIR=${ANGEL_WORKSPACE_DIR}/src/angel_system_nodes/configs
  export MODEL_DIR=${ANGEL_WORKSPACE_DIR}/model_files
  export BAGS_DIR=${ANGEL_WORKSPACE_DIR}/ros_bags
  #export RMW_IMPLEMENTATION=rmw_cyclonedds_cpp

  # Changing the domain ID was important at KHQ to unblock perceived network
  # congestion slowdowns to message sending.
  export ROS_DOMAIN_ID=77
  
  # Set the frame-rate to be used by multiple sources. This should be in frames
  # per second (Hz).
  export FRAME_RATE=15

# Run on project start, the first time
# on_project_first_start: command

# Run on project start, after the first time
# on_project_restart: command

# Run on project exit ( detaching from tmux session )
# on_project_exit: command

# Run on project stop
# on_project_stop: command

# Runs in each window and pane before window/pane specific commands. Useful for setting up interpreter versions.
# pre_window: rbenv shell 2.0.0-p247

# Pass command line options to tmux. Useful for specifying a different tmux.conf.
# tmux_options: -f ~/.tmux.mac.conf
tmux_options: -f <%= ENV["ANGEL_WORKSPACE_DIR"] %>/tmux/tmux.conf

windows:
  - sensor_input:
      layout: even-vertical
      panes:
        - datahub: ros2 run ros_tcp_endpoint default_server_endpoint --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p ROS_IP:=0.0.0.0
        - hl2ss_bridge: ros2 run angel_system_nodes hl2ss_ros_bridge --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p ip_addr:=${HL2_IP}
            -p image_topic:=PVFramesBGR
            -p image_ts_topic:=PVFramesBGR_TS
            -p hand_pose_topic:=disable
            -p audio_topic:=HeadsetAudioData
            -p sm_topic:=disable
            -p head_pose_topic:=disable
            -p pv_width:=1280
            -p pv_height:=720
            -p pv_framerate:=${FRAME_RATE}
            -p sm_freq:=5
            -p rm_depth_AHAT:=disable

        # Visualize RGB Images being output from the headset
        #- rqt_rgb_images: rqt -s rqt_image_view/ImageView
        #    --args ${ROS_NAMESPACE}/PVFramesBGR
        #    --ros-args -p _image_transport:=raw
        
  - vocal:
      layout: even-vertical
      panes:
        - vad: ros2 run angel_system_nodes voice_activity_detector --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p input_audio_topic:=HeadsetAudioData
            -p output_voice_activity_topic:=DetectedVoiceData
            -p vad_server_url:=http://localhost:55667/vad
            -p vad_cadence:=4
            -p vad_margin:=0.50
            -p max_accumulation_length:=4
            -p debug_mode:=True
        - asr: ros2 run angel_system_nodes asr --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p audio_topic:=DetectedVoiceData
            -p utterances_topic:=utterances_topic
            -p asr_server_url:=http://localhost:55667/asr
            -p asr_req_segment_duration:=2
            -p is_sentence_tokenize:=False
            -p debug_mode:=True     
  - intent_detection:
      layout: even-vertical
      panes:
        - intent_detection: ros2 run angel_system_nodes gpt_intent_detector --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p input_topic:=utterances_topic
           -p expect_user_intent_topic:=expect_user_intent_topic
           -p interp_user_intent_topic:=interp_user_intent_topic
           -p timeout:=2
  - emotion_detection:
      layout: even-vertical
      panes:
#        - base_emotion_detection: ros2 run angel_system_nodes base_emotion_detector --ros-args
#           -r __ns:=${ROS_NAMESPACE}
#           -p input_topic:=utterances_topic
#           -p user_emotion_topic:=base_emotion_topic
#        - gpt_emotion_detection: ros2 run angel_system_nodes gpt_emotion_detector --ros-args
#           -r __ns:=${ROS_NAMESPACE}
#           -p input_topic:=utterances_topic
#           -p user_emotion_topic:=gpt_emotion_topic
#           -p timeout:=2
        - emotion_detection: ros2 run angel_system_nodes gpt_emotion_detector --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p input_topic:=interp_user_intent_topic
           -p user_emotion_topic:=emotion_topic
           -p timeout:=2
  - question_answering:
      layout: even-vertical
      panes:
        - gpt_qa: ros2 run angel_system_nodes visual_question_answerer --log-level visual_question_answerer:=DEBUG --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p utterance_topic:=emotion_topic
           -p task_state_topic:=TaskUpdates
           -p object_detections_topic:=ObjectDetections2d
           -p action_classifications_topic:=ActivityDetections
           -p system_text_response_topic:=system_text_response_topic
           -p recipe_path:=${NODE_CONFIG_DIR}/mit_ll_eval_one_coffee_recipe_steps_v2.json
           -p prompt_template_path:=${NODE_CONFIG_DIR}/llm_prompts/vis_qa_teacher_prompt
           -p pv_width:=1920
           -p pv_height:=1080
           -p obj_det_last_n:=8
           -p must_contain_target_phrase:=True
           -p object_det_ignored_objects:="hand (left),hand (right),microwave (closed),microwave (open),background,trash can,peanut butter,nut butter jar lid,nut butter jar (open),nut butter jar (closed),jelly jar lid,jelly jar (open),jelly jar (closed),butter knife + nut butter,butter knife + jelly,tortilla + nut butter,tortilla + jelly"
           -p debug_mode:=True
           
  - object_detector:
      layout: even-vertical
      panes:
        - object_detector: ros2 run angel_system_nodes object_detection_yolo_v7 --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p image_topic:=PVFramesBGR
            -p det_topic:=ObjectDetections2d
            -p net_checkpoint:=${MODEL_DIR}/all_recipes+additional_objs+bkgd_yolov7.pt
            -p inference_img_size:=1280
            -p det_conf_threshold:=0.1
            -p cuda_device_id:=0

        - simple_2d_overlay: ros2 run angel_debug Simple2dDetectionOverlay --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p topic_input_images:=PVFramesBGR
            -p topic_input_det_2d:=ObjectDetections2d
            -p topic_output_images:=pv_image_detections_2d
            -p filter_top_k:=-1

  - activity_classifier: ros2 run angel_system_nodes activity_classifier_tcn --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p image_ts_topic:=PVFramesBGR_TS
      -p det_topic:=ObjectDetections2d
      -p act_topic:=ActivityDetections
      -p model_weights:=${MODEL_DIR}/yolo_all_recipes_additional_objs_bkgd_sample_rate_2.ckpt
      -p model_mapping:=${MODEL_DIR}/yolo_all_recipes_additional_objs_bkgd_act_mapping.txt
      -p model_det_label_mapping:=${MODEL_DIR}/activity_tcn-all_activities-det_label_mapping.json
      -p model_device:=cuda
      -p model_dets_conv_version:=5
      -p window_size:=30
      -p buffer_max_size_seconds:=5
      -p image_pix_width:=1280
      -p image_pix_height:=720

  - keyboard_sys_cmd: ros2 run angel_system_nodes keyboard_to_sys_cmd --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p system_command_topic:=SystemCommands
      
  - task_monitor: ros2 run angel_system_nodes global_step_predictor --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p det_topic:=ActivityDetections
      -p model_file:=${MODEL_DIR}/global_step_predictor_act_avgs_all_classes_v2.0_sample_rate_2.npy
      -p threshold_multiplier_weak:=0.05
      -p thresh_frame_count:=$((8 / (30 / ${FRAME_RATE})))
      -p threshold_frame_count_weak:=2
      -p deactivate_thresh_frame_count:=$((20 / (30 / ${FRAME_RATE})))
      -p step_mode:=granular
      -p config_file:=${CONFIG_DIR}/tasks/multi-task-config.yaml
      -p task_state_topic:=TaskUpdates
      -p query_task_graph_topic:=query_task_graph
      -p task_error_topic:=TaskErrors
      -p system_command_topic:=SystemCommands
      #-p gt_activity_mscoco:=model_files/test_activity_preds.mscoco.json
      #-p gt_video_id:=8
      #-p gt_output_dir:="${BAGS_DIR}"

  - feedback_generator: ros2 run angel_system_nodes feedback_generator --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p activity_detector_topic:=ActivityDetections
      -p object_detection_topic:=ObjectDetections3d
      -p task_monitor_topic:=TaskUpdates
      -p arui_update_topic:=AruiUpdates
      -p utterances_topic:=utterances_topic
      -p interp_user_intent_topic:=InterpUserIntents
      -p system_text_response_topic:=system_text_response_topic

  - engineering-ui:
      layout: even-vertical
      panes:
        - engineering_ui_websocket: ros2 launch rosbridge_server rosbridge_websocket_launch.xml port:=9090
        - engineering_ui_server: node src/angel_utils/multi_task_demo_ui/index.js
            --namespace=${ROS_NAMESPACE}
            --image_topic=pv_image_detections_2d/compressed
            --query_task_graph_topic=query_task_graph
            --task_updates_topic=TaskUpdates
            --activity_detections_topic=ActivityDetections
            --task_errors_topic=TaskErrors
