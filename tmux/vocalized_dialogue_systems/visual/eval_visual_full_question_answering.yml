#
# Used to evaluate Question Answering with visual + vocal processing for a specified ROS bag of data 
# This configuration should be run by itself (e.g. not in combination with
# another tmuxinator launch).
#
# NOTE: In order to query GPT, you will need to execute
# ```
# export OPENAI_API_KEY="YOUR API KEY"
# export OPENAI_ORG_ID="YOUR ORG ID"
# ```
#

name: Visual Question Answering
root: <%= ENV["ANGEL_WORKSPACE_DIR"] %>

# Optional tmux socket
# socket_name: foo

# Note that the pre and post options have been deprecated and will be replaced by
# project hooks.

# Project hooks

# Runs on project start, always
# on_project_start: command
on_project_start: |
  export ROS_NAMESPACE=${ROS_NAMESPACE:-/debug}
  export HL2_IP=${HL2_IP:-192.168.1.101}
  export CONFIG_DIR=${ANGEL_WORKSPACE_DIR}/src/angel_system_nodes/configs
  export MODEL_DIR=${ANGEL_WORKSPACE_DIR}/model_files
  export BERKELEY_CONFIG_DIR=${ANGEL_WORKSPACE_DIR}/angel_system/berkeley/configs
# Run on project start, the first time
# on_project_first_start: command

# Run on project start, after the first time
# on_project_restart: command

# Run on project exit ( detaching from tmux session )
# on_project_exit: command

# Run on project stop
# on_project_stop: command

# Runs in each window and pane before window/pane specific commands. Useful for setting up interpreter versions.
# pre_window: rbenv shell 2.0.0-p247

# Pass command line options to tmux. Useful for specifying a different tmux.conf.
# tmux_options: -f ~/.tmux.mac.conf
tmux_options: -f <%= ENV["ANGEL_WORKSPACE_DIR"] %>/tmux/tmux.conf

# Change the command to call tmux.  This can be used by derivatives/wrappers like byobu.
# tmux_command: byobu

# Specifies (by name or index) which window will be selected on project startup. If not set, the first window is used.
# startup_window: editor

# Specifies (by index) which pane of the specified window will be selected on project startup. If not set, the first pane is used.
# startup_pane: 1

# Controls whether the tmux session should be attached to automatically. Defaults to true.
# attach: false

windows:
#   - datahub: ros2 run ros_tcp_endpoint default_server_endpoint --ros-args
#       -r __ns:=${ROS_NAMESPACE}
#       -p ROS_IP:=0.0.0.0
#   - hl2ss_bridge: ros2 run angel_system_nodes hl2ss_ros_bridge --ros-args
#       -r __ns:=${ROS_NAMESPACE}
#       -p ip_addr:=${HL2_IP}
#       -p image_topic:=PVFramesBGR
#       -p image_ts_topic:=disable
#       -p hand_pose_topic:=disable
#       -p audio_topic:=HeadsetAudioData
#       -p head_pose_topic:=HeadsetPoseData
#       -p sm_topic:=disable
#       -p rm_depth_AHAT:=disable
#       -p pv_width:=760
#       -p pv_height:=428
#       -p pv_framerate:=30
#       -p sm_freq:=5
  - sensor_input:
      layout: even-vertical
      panes:
        - ros_bag_play: sleep  2; ros2 bag play ros_bags/josh_rosbag/josh_rosbag.db3

        # Old videos were recorded in NV12
        #- image_converter: ros2 run angel_datahub ImageConverter --ros-args
        #    -r __ns:=${ROS_NAMESPACE}
        #    -p topic_input_images:=PVFramesNV12
        #    -p topic_output_images:=PVFramesRGB

        - image_ts_relay: ros2 run angel_system_nodes  image_timestamp_relay --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p image_topic:=PVFramesRGB
            -p output_topic:=PVFramesRGB_TS

        # Visualize RGB Images being output from the headset
        - rqt_rgb_images: rqt -s rqt_image_view/ImageView
            --args ${ROS_NAMESPACE}/PVFramesBGR
            --ros-args -p _image_transport:=raw
  - object_detector:
      layout: even-vertical
      panes:
        - berkeley_object_detector: ros2 run angel_system_nodes berkeley_object_detector --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p image_topic:=PVFramesBGR
            -p det_topic:=ObjectDetections2d
            -p det_conf_threshold:=0.1
            -p cuda_device_id:=0
            -p model_config:=${BERKELEY_CONFIG_DIR}/MC50-InstanceSegmentation/cooking/coffee/stage2/mask_rcnn_R_50_FPN_1x_demo.yaml
        # - object_detector: ros2 run angel_system_nodes object_detection_yolo_v7 --ros-args
        #     -r __ns:=${ROS_NAMESPACE}
        #     -p image_topic:=PVFramesBGR
        #     -p det_topic:=ObjectDetections2d
        #     -p net_checkpoint:=${MODEL_DIR}/yolov7-combined_objects-weights.pt
        #     -p inference_img_size:=1280
        #     -p det_conf_threshold:=0.5
        #     -p cuda_device_id:=0
        - simple_2d_overlay: ros2 run angel_debug Simple2dDetectionOverlay --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p topic_input_images:=PVFramesBGR
            -p topic_input_det_2d:=ObjectDetections2d
            -p topic_output_images:=pv_image_detections_2d
            -p filter_top_k:=-1
  - activity_classifier: ros2 run angel_system_nodes activity_classifier_tcn --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p image_ts_topic:=PVFramesBGR_TS
      -p det_topic:=ObjectDetections2d
      -p act_topic:=ActivityDetections
      -p model_weights:=${MODEL_DIR}/activity_tcn-coffee-checkpoint.ckpt
      -p model_mapping:=${MODEL_DIR}/activity_tcn-coffee-mapping.txt
      -p model_det_label_mapping:=${MODEL_DIR}/activity_tcn-coffee-det_label_mapping.json
      -p model_device:=cuda:0
      -p model_dets_conv_version:=5
      -p window_size:=30
      -p buffer_max_size_seconds:=5
      -p image_pix_width:=1280
      -p image_pix_height:=720
  - multi_task_monitor: ros2 run angel_system_nodes dummy_multi_task_monitor --ros-args
      -r __ns:=${ROS_NAMESPACE}
      -p config_file:=${CONFIG_DIR}/tasks/multi-task-config.yaml
      -p task_state_topic:=task_state_topic
      -p task_error_topic:=TaskErrors
      -p query_task_graph_topic:=query_task_graph
      -p sys_cmd_topic:=SystemCommands
  - vocal:
      layout: even-vertical
      panes:
        - vad: ros2 run angel_system_nodes voice_activity_detector --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p input_audio_topic:=HeadsetAudioData
            -p output_voice_activity_topic:=DetectedVoiceData
            -p vad_server_url:=http://communication.cs.columbia.edu:55667/vad
            -p vad_cadence:=4
            -p vad_margin:=0.50
            -p max_accumulation_length:=15
            -p debug_mode:=True
        - asr: ros2 run angel_system_nodes asr --ros-args
            -r __ns:=${ROS_NAMESPACE}
            -p audio_topic:=DetectedVoiceData
            -p utterances_topic:=utterances_topic
            -p asr_server_url:=http://communication.cs.columbia.edu:55667/asr
            -p asr_req_segment_duration:=2
            -p is_sentence_tokenize:=False
            -p debug_mode:=True
  - intent_detection:
      layout: even-vertical
      panes:
        - intent_detection: ros2 run angel_system_nodes gpt_intent_detector --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p input_topic:=utterances_topic
           -p expect_user_intent_topic:=expect_user_intent_topic
           -p interp_user_intent_topic:=interp_user_intent_topic
  - emotion_detection:
      layout: even-vertical
      panes:
        - emotion_detection: ros2 run angel_system_nodes base_emotion_detector --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p input_topic:=interp_user_intent_topic
           -p user_emotion_topic:=emotion_topic
  - question_answering:
      layout: even-vertical
      panes:
        - gpt_qa: ros2 run angel_system_nodes visual_question_answerer --ros-args
           -r __ns:=${ROS_NAMESPACE}
           -p utterance_topic:=emotion_topic
           -p task_state_topic:=task_state_topic
           -p object_detections_topic:=ObjectDetections2d
           -p action_classifications_topic:=ActivityDetections
           -p system_text_response_topic:=system_text_response_topic
           -p recipe_path:=${CONFIG_DIR}/mit_ll_eval_one_coffee_recipe_steps_v2.json
           -p prompt_template_path:=${CONFIG_DIR}/llm_prompts/vis_qa_teacher_prompt
           -p obj_det_last_n:=5
           -p pv_width:=1920
           -p pv_height:=1080
           -p debug_mode:=True